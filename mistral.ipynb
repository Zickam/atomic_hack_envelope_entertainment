{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install PyPDF2, langchain==0.2.4, langchain_community==0.2.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34c3c8ddeaaa5ef0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPyPDF2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PdfReader\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext_splitter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvectorstores\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FAISS\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_community\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HuggingFaceEmbeddings\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T11:22:53.817827800Z",
     "start_time": "2024-06-15T11:22:53.785891500Z"
    }
   },
   "id": "56352eebe9f0f3c4",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mintfloat/multilingual-e5-large\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m----> 3\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m HuggingFaceEmbeddings(\n\u001B[0;32m      4\u001B[0m     model_name\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[0;32m      5\u001B[0m     model_kwargs\u001B[38;5;241m=\u001B[39mmodel_kwargs\n\u001B[0;32m      6\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'HuggingFaceEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "model_id = 'intfloat/multilingual-e5-large'\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T11:22:55.256111300Z",
     "start_time": "2024-06-15T11:22:55.236527300Z"
    }
   },
   "id": "397c57d8c202ca7f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_docs):\n",
    "    text = \" \"\n",
    "    # Iterate through each PDF document path in the list\n",
    "    for pdf in pdf_docs:\n",
    "        # Create a PdfReader object for the current PDF document\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        # Iterate through each page in the PDF document\n",
    "        for page in pdf_reader.pages:\n",
    "            # Extract text from the current page and append it to the 'text' string\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Return the concatenated text from all PDF documents\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_store(text_chunks):     \n",
    "\n",
    "    # Create a vector store using FAISS from the provided text chunks and embeddings\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "\n",
    "    # Save the vector store locally with the name \"faiss_index\"\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_conversational_chain():\n",
    "    # Define a prompt template for asking questions based on a given context\n",
    "    prompt_template = \"\"\"\n",
    "    Ты эксперт по базе знаний компании РОСАТОМ. Пользователь задает тебе вопрос, ты отвечаешь на него в чате, используя только контекст базы знаний компании, предоставленной тебе в виде файлов. Твоя задача помочь \n",
    "    пользователю найти ответ на вопрос, и решить его. Все ответы пиши в контексте процессов, регламентов и документов компании РОСАТОМ. Пользователь это сотрудник компании, твоя задача максимально точно и понятно \n",
    "    отвечать на вопросы пользователя. Помни, пользователь задает вопросы только в контексте базы знаний компании РОСАТОМ. Отвечай только на заданный вопрос. Не делай повторов предложений. Отвечай так, чтобы \n",
    "    пользователь мог самостоятельно решить свой вопрос, используя, приложение, документацию, или сайт компании. В ответе описывай шаги по решению вопроса, если оно есть в базе знаний. Не предлагай \n",
    "    пользователю обращаться в техническую поддержку, ты и есть часть технической поддержки. То чего нет в базе знаний компании ты не знаешь, и ответить не можешь. Никогда не пиши вопрос решен, опиши шаги \n",
    "    необходимые для решения вопроса.\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a ChatGoogleGenerativeAI model for conversational AI\n",
    "    model = LlamaCpp(\n",
    "    model_path=\"atomic_hack_envelope_entertainment\\\\data\\\\model\\\\model-q4_K.gguf\",  # Путь к модели\n",
    "    temperature=0.1,  # Температура для управления степенью случайности в ответах\n",
    "    max_tokens=512,  # Максимальное количество токенов в ответе\n",
    "    max_length=1000,  # Максимальная длина текста (в символах)\n",
    "    # callback_manager=callback_manager,  # Менеджер коллбэков\n",
    "    f16_kv=True,\n",
    "    n_batch=512,\n",
    "    verbose=False,  # Отключение подробного вывода  # Увеличиваем максимальное количество новых токенов\n",
    "    n_ctx=35000,\n",
    "    n_gpu_layers=-1,  # -1 для максимального использования GPU, 0 для CPU\n",
    ")\n",
    "\n",
    "    # Create a prompt template with input variables \"context\" and \"question\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # Load a question-answering chain with the specified model and prompt\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "def user_input(user_question):\n",
    "    # Create embeddings for the user question using a Google Generative AI model\n",
    "\n",
    "    # Load a FAISS vector database from a local file\n",
    "    new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = new_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "    print(retriever)\n",
    "    # Perform similarity search in the vector database based on the user question\n",
    "    docs = new_db.similarity_search(user_question)\n",
    "    print(docs)\n",
    "    # Obtain a conversational question-answering chain\n",
    "    chain = get_conversational_chain()\n",
    "\n",
    "    # Use the conversational chain to get a response based on the user question and retrieved documents\n",
    "    response = chain(\n",
    "        {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
    "    )\n",
    "\n",
    "    # Print the response to the console\n",
    "    print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f00cb622726de1ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_input('Оплата')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f040af6f76824bb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
